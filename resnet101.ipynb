{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO5q0mntepDi",
        "outputId": "237ee74e-04a5-4c96-d747-a06109df3553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the paths for your training, validation, and test datasets\n",
        "train_dir = '/content/drive/MyDrive/data/sdss_dataset_split/Train'\n",
        "val_dir = '/content/drive/MyDrive/data/sdss_dataset_split/Validation'\n",
        "test_dir = '/content/drive/MyDrive/data/sdss_dataset_split/Test'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.applications import ResNet101\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Define image size and batch size\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Load the ResNet101 model without the top layer\n",
        "base_model = ResNet101(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "# Unfreeze the last few layers for fine-tuning\n",
        "for layer in base_model.layers[-8:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(3, activation='softmax')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwkUG_zrolUf",
        "outputId": "67cd9d0a-83e4-4db9-c9be-cd652726fc6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m171446536/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with a learning rate and optimizer\n",
        "optimizer = Adam(learning_rate=1e-5)  # You can switch to SGD if needed\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "FyIdHYPsonH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and data generators for loading images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2S0O7n2ous5",
        "outputId": "fe563a29-b751-467f-d08a-332ba4101233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5240 images belonging to 3 classes.\n",
            "Found 650 images belonging to 3 classes.\n",
            "Found 650 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks for learning rate reduction, early stopping, and model checkpoint\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1, restore_best_weights=True)\n",
        "checkpoint_filepath = '/content/drive/MyDrive/best_resnet101_model.keras'\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,  # Adjust epochs as needed\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[reduce_lr, early_stopping, model_checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHswlTXUowwu",
        "outputId": "3ab00982-f827-4b8c-9893-630306f6a7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - accuracy: 0.8335 - loss: 0.4218\n",
            "Epoch 1: val_accuracy improved from -inf to 0.82462, saving model to /content/drive/MyDrive/best_resnet101_model.keras\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 574ms/step - accuracy: 0.8334 - loss: 0.4218 - val_accuracy: 0.8246 - val_loss: 0.4207 - learning_rate: 2.5000e-06\n",
            "Epoch 2/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500ms/step - accuracy: 0.8383 - loss: 0.4184\n",
            "Epoch 2: val_accuracy improved from 0.82462 to 0.82615, saving model to /content/drive/MyDrive/best_resnet101_model.keras\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 580ms/step - accuracy: 0.8383 - loss: 0.4184 - val_accuracy: 0.8262 - val_loss: 0.4118 - learning_rate: 2.5000e-06\n",
            "Epoch 3/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - accuracy: 0.8579 - loss: 0.3930\n",
            "Epoch 3: val_accuracy improved from 0.82615 to 0.83077, saving model to /content/drive/MyDrive/best_resnet101_model.keras\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 574ms/step - accuracy: 0.8578 - loss: 0.3932 - val_accuracy: 0.8308 - val_loss: 0.4014 - learning_rate: 2.5000e-06\n",
            "Epoch 4/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - accuracy: 0.8354 - loss: 0.4103\n",
            "Epoch 4: val_accuracy did not improve from 0.83077\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 505ms/step - accuracy: 0.8354 - loss: 0.4103 - val_accuracy: 0.8262 - val_loss: 0.4048 - learning_rate: 2.5000e-06\n",
            "Epoch 5/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.8255 - loss: 0.4191\n",
            "Epoch 5: val_accuracy did not improve from 0.83077\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 511ms/step - accuracy: 0.8255 - loss: 0.4191 - val_accuracy: 0.8262 - val_loss: 0.4108 - learning_rate: 2.5000e-06\n",
            "Epoch 6/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.8357 - loss: 0.4177\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
            "\n",
            "Epoch 6: val_accuracy did not improve from 0.83077\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 509ms/step - accuracy: 0.8357 - loss: 0.4177 - val_accuracy: 0.8246 - val_loss: 0.4159 - learning_rate: 2.5000e-06\n",
            "Epoch 7/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.8459 - loss: 0.3909\n",
            "Epoch 7: val_accuracy improved from 0.83077 to 0.83538, saving model to /content/drive/MyDrive/best_resnet101_model.keras\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 591ms/step - accuracy: 0.8459 - loss: 0.3909 - val_accuracy: 0.8354 - val_loss: 0.3938 - learning_rate: 1.2500e-06\n",
            "Epoch 8/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - accuracy: 0.8460 - loss: 0.3998\n",
            "Epoch 8: val_accuracy improved from 0.83538 to 0.83846, saving model to /content/drive/MyDrive/best_resnet101_model.keras\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 576ms/step - accuracy: 0.8460 - loss: 0.3998 - val_accuracy: 0.8385 - val_loss: 0.3917 - learning_rate: 1.2500e-06\n",
            "Epoch 9/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504ms/step - accuracy: 0.8489 - loss: 0.3825\n",
            "Epoch 9: val_accuracy did not improve from 0.83846\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 517ms/step - accuracy: 0.8489 - loss: 0.3827 - val_accuracy: 0.8323 - val_loss: 0.3946 - learning_rate: 1.2500e-06\n",
            "Epoch 10/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - accuracy: 0.8376 - loss: 0.3938\n",
            "Epoch 10: val_accuracy did not improve from 0.83846\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 505ms/step - accuracy: 0.8377 - loss: 0.3938 - val_accuracy: 0.8308 - val_loss: 0.4016 - learning_rate: 1.2500e-06\n",
            "Epoch 11/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - accuracy: 0.8492 - loss: 0.3919\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
            "\n",
            "Epoch 11: val_accuracy improved from 0.83846 to 0.84154, saving model to /content/drive/MyDrive/best_resnet101_model.keras\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 571ms/step - accuracy: 0.8492 - loss: 0.3918 - val_accuracy: 0.8415 - val_loss: 0.3935 - learning_rate: 1.2500e-06\n",
            "Epoch 12/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496ms/step - accuracy: 0.8428 - loss: 0.3989\n",
            "Epoch 12: val_accuracy did not improve from 0.84154\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 509ms/step - accuracy: 0.8428 - loss: 0.3989 - val_accuracy: 0.8369 - val_loss: 0.3972 - learning_rate: 6.2500e-07\n",
            "Epoch 13/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.8498 - loss: 0.3926\n",
            "Epoch 13: val_accuracy improved from 0.84154 to 0.84308, saving model to /content/drive/MyDrive/best_resnet101_model.keras\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 572ms/step - accuracy: 0.8498 - loss: 0.3925 - val_accuracy: 0.8431 - val_loss: 0.3888 - learning_rate: 6.2500e-07\n",
            "Epoch 14/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - accuracy: 0.8499 - loss: 0.3867\n",
            "Epoch 14: val_accuracy did not improve from 0.84308\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 500ms/step - accuracy: 0.8499 - loss: 0.3867 - val_accuracy: 0.8431 - val_loss: 0.3906 - learning_rate: 6.2500e-07\n",
            "Epoch 15/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - accuracy: 0.8479 - loss: 0.3840\n",
            "Epoch 15: val_accuracy did not improve from 0.84308\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 507ms/step - accuracy: 0.8479 - loss: 0.3840 - val_accuracy: 0.8369 - val_loss: 0.3977 - learning_rate: 6.2500e-07\n",
            "Epoch 16/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.8510 - loss: 0.3808\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
            "\n",
            "Epoch 16: val_accuracy did not improve from 0.84308\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 511ms/step - accuracy: 0.8510 - loss: 0.3809 - val_accuracy: 0.8385 - val_loss: 0.3929 - learning_rate: 6.2500e-07\n",
            "Epoch 17/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - accuracy: 0.8462 - loss: 0.4045\n",
            "Epoch 17: val_accuracy did not improve from 0.84308\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 510ms/step - accuracy: 0.8462 - loss: 0.4044 - val_accuracy: 0.8400 - val_loss: 0.3914 - learning_rate: 3.1250e-07\n",
            "Epoch 18/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - accuracy: 0.8581 - loss: 0.3712\n",
            "Epoch 18: val_accuracy did not improve from 0.84308\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 506ms/step - accuracy: 0.8581 - loss: 0.3712 - val_accuracy: 0.8354 - val_loss: 0.3926 - learning_rate: 3.1250e-07\n",
            "Epoch 19/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500ms/step - accuracy: 0.8475 - loss: 0.3857\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
            "\n",
            "Epoch 19: val_accuracy did not improve from 0.84308\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 513ms/step - accuracy: 0.8475 - loss: 0.3857 - val_accuracy: 0.8292 - val_loss: 0.3929 - learning_rate: 3.1250e-07\n",
            "Epoch 20/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.8506 - loss: 0.3614\n",
            "Epoch 20: val_accuracy did not improve from 0.84308\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 505ms/step - accuracy: 0.8506 - loss: 0.3615 - val_accuracy: 0.8308 - val_loss: 0.3921 - learning_rate: 1.5625e-07\n",
            "Epoch 21/50\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497ms/step - accuracy: 0.8515 - loss: 0.3635\n",
            "Epoch 21: val_accuracy did not improve from 0.84308\n",
            "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 510ms/step - accuracy: 0.8515 - loss: 0.3635 - val_accuracy: 0.8308 - val_loss: 0.3919 - learning_rate: 1.5625e-07\n",
            "Epoch 21: early stopping\n",
            "Restoring model weights from the end of the best epoch: 13.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "id": "2y5_XmzXozFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model\n",
        "final_model_filepath = '/content/drive/MyDrive/final_resnet101_model.keras'\n",
        "model.save(final_model_filepath)\n"
      ],
      "metadata": {
        "id": "NsMqNJ4qo8yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(final_model_filepath)\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess an image\n",
        "img_path = '/content/drive/MyDrive/data/sdss_dataset_split/Test/6/image_102_class_6.jpg'\n",
        "img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Normalize pixel values\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class = np.argmax(predictions, axis=1)\n",
        "print(f'Predicted class index: {predicted_class[0]}')\n",
        "\n",
        "# Define class labels and map to predicted class\n",
        "class_labels = ['Class 5', 'Class 6', 'Class 7']\n",
        "predicted_label = class_labels[predicted_class[0]]\n",
        "print(f'Predicted class label: {predicted_label}')\n"
      ],
      "metadata": {
        "id": "YN_ibwU_o_OD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}